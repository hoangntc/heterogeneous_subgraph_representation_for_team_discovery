{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import collections, itertools\n",
    "import networkx as nx\n",
    "import regex as re\n",
    "\n",
    "PROJ_PATH = os.path.join(re.sub(\"/heterogeneous.*$\", '', os.getcwd()), 'heterogeneous_subgraph_representation_for_team_discovery')\n",
    "sys.path.insert(1, os.path.join(PROJ_PATH, 'src'))\n",
    "\n",
    "from train_config import *\n",
    "from datasets import SubgraphDataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySubGraphs():\n",
    "    '''\n",
    "    Args:\n",
    "        cid2cname: dictionary to map conference id to conference name which is used as the label of subgraph\n",
    "        n_subgraphs: number of subgraph for each label\n",
    "        n_nodes_in_subgraph: number of nodes in each subgraph\n",
    "        tvt (default: 60, 20, 20): ratio for training, validate and testing set\n",
    "        shuffle (default=True): whether to shuffle the order of subgraphs\n",
    "    Return:\n",
    "     A subgraph object with:\n",
    "        sub_G: list of subgraphs\n",
    "        sub_G_label: list of subgraphs label\n",
    "        mask: dataset indicator (whether data used for training/validation/testing)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        PA_edges, \n",
    "        PT_edges,\n",
    "        PC_edges,\n",
    "        cid2cname,\n",
    "        n_subgraphs,\n",
    "        n_nodes_in_subgraph,\n",
    "        use_venue=True,\n",
    "        tvt=[60, 20, 20],\n",
    "        shuffle=True):\n",
    "        \n",
    "        assert sum(tvt) == 100, 'Sum of TVT should be 100!'\n",
    "        \n",
    "        self.PA_edges = PA_edges\n",
    "        self.PT_edges = PT_edges\n",
    "        self.PC_edges = PC_edges\n",
    "        self.edges = pd.concat([self.PA_edges, self.PT_edges, self.PC_edges])\n",
    "        \n",
    "        self.cid2cname = cid2cname\n",
    "        self.n_subgraphs = n_subgraphs\n",
    "        self.n_nodes_in_subgraph = n_nodes_in_subgraph\n",
    "        self.use_venue = use_venue\n",
    "        self.tvt = tvt\n",
    "        self.shuffle = shuffle\n",
    "        self.n_groups = len(cid2cname)\n",
    "        \n",
    "    def create_base_graph(self):\n",
    "        '''\n",
    "        Create a base graph for each label before sampling subgraphs\n",
    "            - cid: conference id\n",
    "            - cname: conference name\n",
    "        '''\n",
    "        G_dict = {}\n",
    "        for cid, cname in self.cid2cname.items():\n",
    "            print(f'- Creating base graph for group {cname}')\n",
    "            # filter paper\n",
    "            PC_edges_filtered = self.PC_edges[self.PC_edges['target']==cid]\n",
    "            papers_filtered = list(PC_edges_filtered['source'].unique())\n",
    "            \n",
    "            # filter author\n",
    "            PA_edges_filtered = self.PA_edges[self.PA_edges['source'].isin(papers_filtered)]\n",
    "            \n",
    "            # filter term\n",
    "            PT_edge_filtered = self.PA_edges[self.PA_edges['source'].isin(papers_filtered)]\n",
    "            \n",
    "            # create networkx graph\n",
    "            if self.use_venue:\n",
    "                df_edges_filtered = pd.concat([PC_edges_filtered, PA_edges_filtered, PT_edge_filtered])\n",
    "            else:\n",
    "                df_edges_filtered = pd.concat([PA_edges_filtered, PT_edge_filtered])\n",
    "            G_i = nx.from_pandas_edgelist(df_edges_filtered)\n",
    "            G_dict[cname] = G_i\n",
    "            \n",
    "        return G_dict\n",
    "    \n",
    "    def get_subgraphs_randomly(self, G_dict):\n",
    "        \"\"\"\n",
    "        Randomly generates subgraphs of size n_nodes_in_subgraph\n",
    "        Args\n",
    "            - n_subgraphs (int): number of subgraphs\n",
    "            - n_nodes_in_subgraph (int): number of nodes in each subgraph\n",
    "        Return\n",
    "            - subgraphs (list of lists): list of subgraphs, where each subgraph is a list of nodes\n",
    "        \"\"\"\n",
    "\n",
    "        sub_G = []\n",
    "        sub_G_label = []\n",
    "        random.seed(0)\n",
    "        \n",
    "        print(f'- Sampling {self.n_subgraphs*self.n_groups} subgraphs')\n",
    "        for cname, G in G_dict.items():\n",
    "            print(f'- Sampling {self.n_subgraphs} subgraphs for group {cname}')\n",
    "            for s in range(self.n_subgraphs):\n",
    "                n_nodes = min(len(G.nodes), self.n_nodes_in_subgraph)\n",
    "                sampled_nodes = random.sample(G.nodes, n_nodes)\n",
    "                sub_G.append(sampled_nodes)\n",
    "                sub_G_label.append(cname)\n",
    "                \n",
    "        if self.shuffle:\n",
    "            tmp = list(zip(sub_G, sub_G_label))\n",
    "            random.shuffle(tmp)\n",
    "            sub_G, sub_G_label = zip(*tmp)\n",
    "        return sub_G, sub_G_label\n",
    "     \n",
    "    def get_train_val_test(self):\n",
    "        self.n_samples = self.n_subgraphs * self.n_groups\n",
    "        n_train = int(self.n_samples * self.tvt[0] / 100)\n",
    "        n_val = int(self.n_samples * self.tvt[1] / 100)\n",
    "        n_test = self.n_samples - n_train - n_val\n",
    "        mask = [0] * n_train + [1] * n_val + [2] * n_test\n",
    "        return mask\n",
    "    \n",
    "    def sample_subgraphs(self):\n",
    "        print('Creating base graphs')\n",
    "        self.G_dict = self.create_base_graph()\n",
    "        print('Sampling subgraphs')\n",
    "        self.sub_G, self.sub_G_label = self.get_subgraphs_randomly(self.G_dict)\n",
    "        self.mask = self.get_train_val_test()  \n",
    "\n",
    "        \n",
    "class MySubGraphs_v2():\n",
    "    '''\n",
    "    Args:\n",
    "        cid2cname: dictionary to map conference id to conference name which is used as the label of subgraph\n",
    "        n_subgraphs: number of subgraph for each label\n",
    "        n_nodes_in_subgraph: number of nodes in each subgraph\n",
    "        tvt (default: 60, 20, 20): ratio for training, validate and testing set\n",
    "        shuffle (default=True): whether to shuffle the order of subgraphs\n",
    "    Return:\n",
    "     A subgraph object with:\n",
    "        sub_G: list of subgraphs\n",
    "        sub_G_label: list of subgraphs label\n",
    "        mask: dataset indicator (whether data used for training/validation/testing)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        PA_edges,\n",
    "        PT_edges,\n",
    "        PC_edges, \n",
    "        cid2cname,\n",
    "        ds2pids,\n",
    "        remove_pa_in_test=True,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "    ):\n",
    "        \n",
    "        self.PA_edges = PA_edges\n",
    "        self.PT_edges = PT_edges\n",
    "        self.PC_edges = PC_edges\n",
    "        self.paper_nodes = list(PA_edges['source'].unique())\n",
    "        self.author_nodes = list(PA_edges['target'].unique())\n",
    "        self.term_nodes = list(PT_edges['target'].unique())\n",
    "        self.conference_nodes = list(PC_edges['target'].unique())\n",
    "        \n",
    "        self.cid2cname = cid2cname # conference id to conference name\n",
    "        self.ds2pids = ds2pids\n",
    "        self.train_pids = ds2pids['train']\n",
    "        self.val_pids = ds2pids['val']\n",
    "        self.test_pids = ds2pids['test']\n",
    "        self.remove_pa_in_test = remove_pa_in_test\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.n_groups = len(cid2cname)\n",
    "        print(f'Number of train: {len(self.train_pids)}')\n",
    "        print(f'Number of val: {len(self.val_pids)}')\n",
    "        print(f'Number of test: {len(self.test_pids)}')\n",
    "        \n",
    "    def get_subgraphs_based_on_paper(self):\n",
    "        \"\"\"\n",
    "        Randomly generates subgraphs of size n_nodes_in_subgraph\n",
    "        Args\n",
    "            - n_subgraphs (int): number of subgraphs\n",
    "            - n_nodes_in_subgraph (int): number of nodes in each subgraph\n",
    "        Return\n",
    "            - subgraphs (list of lists): list of subgraphs, where each subgraph is a list of nodes\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        sub_G = []\n",
    "        sub_G_label = []\n",
    "        mask_G = []\n",
    "        id_G = []\n",
    "        \n",
    "        print(f'- Sampling {len(self.paper_nodes)} subgraphs')\n",
    "        for pid in self.paper_nodes:\n",
    "            PC_edges_filtered = self.PC_edges[self.PC_edges['source']==pid]\n",
    "            cids = PC_edges_filtered['target'].unique().tolist()\n",
    "            cnames = '-'.join([cid2cname[i].replace('-', '') for i in cids])\n",
    "            \n",
    "            sampled_paper_nodes = [pid]\n",
    "            id_G.append(pid) # for prediction phase\n",
    "            sampled_term_nodes = list(self.PT_edges[self.PT_edges['source'].isin(sampled_paper_nodes)]['target'].unique())\n",
    "            sampled_author_nodes = list(self.PA_edges[self.PA_edges['source'].isin(sampled_paper_nodes)]['target'].unique())\n",
    "         \n",
    "            if pid in self.test_pids and self.remove_pa_in_test:\n",
    "                sampled_nodes = sampled_paper_nodes + sampled_term_nodes\n",
    "            else:        \n",
    "                sampled_nodes = sampled_author_nodes + sampled_paper_nodes + sampled_term_nodes\n",
    "\n",
    "            sub_G.append(sampled_nodes)\n",
    "            sub_G_label.append(cnames)\n",
    "\n",
    "            if pid in self.train_pids:\n",
    "                mask_G.append(0)\n",
    "            elif pid in self.val_pids:\n",
    "                mask_G.append(1)\n",
    "            elif pid in self.test_pids:\n",
    "                mask_G.append(2)\n",
    "\n",
    "        if self.shuffle:\n",
    "            print('-- Shuffle data')\n",
    "            tmp = list(zip(sub_G, sub_G_label, mask_G, id_G))\n",
    "            random.shuffle(tmp)\n",
    "            tmp = sorted(tmp, key=lambda x: x[2])\n",
    "            sub_G, sub_G_label, mask_G, id_G = zip(*tmp)\n",
    "        return sub_G, sub_G_label, mask_G, id_G\n",
    "    \n",
    "    def get_node_types(self):\n",
    "        # author --> paper --> term --> conference\n",
    "        no_author = len(self.author_nodes)\n",
    "        no_paper = len(self.paper_nodes)\n",
    "        no_term = len(self.term_nodes)\n",
    "        node_id = range(no_author + no_paper + no_term)\n",
    "        node_type = [0] * no_author + [1] * no_paper + [2] * no_term\n",
    "        nodetype_mapping = {\n",
    "            0: 'author',\n",
    "            1: 'paper',\n",
    "            2: 'term',\n",
    "        }\n",
    "        df_node_types = pd.DataFrame({'node_id': node_id, 'node_type': node_type})\n",
    "        df_node_types['node_type_name'] = df_node_types['node_type'].map(nodetype_mapping)\n",
    "        return df_node_types\n",
    "\n",
    "    def get_edges(self):\n",
    "        if self.remove_pa_in_test:\n",
    "            self.PA_edges_filtered = self.PA_edges[~self.PA_edges['target'].isin(self.test_pids)]\n",
    "        else:\n",
    "            self.PA_edges_filtered = self.PA_edges.copy()\n",
    "        df_edges = pd.concat([self.PT_edges, self.PA_edges_filtered])\n",
    "        print(f'Number of edges {df_edges.shape[0]}')\n",
    "        return df_edges\n",
    "        \n",
    "    def sample_subgraphs(self):\n",
    "        print('Start sampling subgraphs ...')\n",
    "        self.sub_G, self.sub_G_label, self.mask_G, self.id_G = self.get_subgraphs_based_on_paper()\n",
    "        \n",
    "        print('Start getting nodes ...')\n",
    "        self.df_node_types = self.get_node_types()\n",
    "        \n",
    "        print('Start getting edges ...')\n",
    "        self.df_edges = self.get_edges()\n",
    "                 \n",
    "def write_subgraph(sub_f, sub_G, sub_G_label, mask):\n",
    "    \"\"\"\n",
    "    Write subgraph information into the appropriate format for HSGNN (tab-delimited file where each row\n",
    "    has dash-delimited nodes, subgraph label, and train/val/test label).\n",
    "    Args\n",
    "        - sub_f (str): file directory to save subgraph information\n",
    "        - sub_G (list of lists): list of subgraphs, where each subgraph is a list of nodes\n",
    "        - sub_G_label (list): subgraph labels\n",
    "        - mask (list): 0 if subgraph is in train set, 1 if in val set, 2 if in test set\n",
    "    \"\"\"\n",
    "\n",
    "    with open(sub_f, \"w\") as fout:\n",
    "        for g, l, m in zip(sub_G, sub_G_label, mask):\n",
    "            g = [str(val) for val in g]\n",
    "            if len(g) == 0: continue\n",
    "            if m == 0: fout.write(\"\\t\".join([\"-\".join(g), str(l), \"train\", \"\\n\"]))\n",
    "            elif m == 1: fout.write(\"\\t\".join([\"-\".join(g), str(l), \"val\", \"\\n\"]))\n",
    "            elif m == 2: fout.write(\"\\t\".join([\"-\".join(g), str(l), \"test\", \"\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n"
     ]
    }
   ],
   "source": [
    "folds = pd.read_pickle('/media/HSGNN/dataset/Train_Test_indices_V2.2.pkl')\n",
    "entityID_map = np.genfromtxt('/media/HSGNN/dataset/V2_2/entity_id_mapping.csv', delimiter=\",\", dtype=str)\n",
    "paperIDmap = {int(i[2]):int(i[3]) for i in entityID_map[1:] if i[1]==\"paper\"}\n",
    "print(folds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 3579\n",
    "random.seed(seed)\n",
    "val_ratio = 0.05\n",
    "n_folds = 10\n",
    "for i in range(1, n_folds+1):\n",
    "    print('############################################################')\n",
    "    print(f'Prepare tvt file for fold: {i}')\n",
    "    experiment = f'fold_{i}'\n",
    "    train_ids_origin = folds[i]['Train']\n",
    "    test_ids_origin = folds[i]['Test']\n",
    "    trainval_ids = [paperIDmap[idx] for idx in train_ids_origin]\n",
    "    test_pids = [paperIDmap[idx] for idx in test_ids_origin]\n",
    "    random.shuffle(trainval_ids)\n",
    "    n_trainval = len(trainval_ids)\n",
    "    n_val = int(n_trainval * val_ratio)\n",
    "    train_pids = trainval_ids[:-n_val]\n",
    "    val_pids = trainval_ids[-n_val:]\n",
    "    \n",
    "    assert min(train_pids+val_pids+test_pids) == 1840\n",
    "    assert max(train_pids+val_pids+test_pids) == (1840+10674-1)\n",
    "    assert len(set(train_pids+val_pids+test_pids)) == 10674\n",
    "    \n",
    "    fold_dir = os.path.join(PROJ_PATH, 'dataset', experiment)\n",
    "    if not os.path.exists(fold_dir): \n",
    "        os.mkdir(fold_dir)\n",
    "    print(f'Number of train: {len(train_pids)}')\n",
    "    print(f'Number of val: {len(val_pids)}')\n",
    "    print(f'Number of test: {len(test_pids)}')\n",
    "    ds2pids = {\n",
    "        'train': train_pids,\n",
    "        'val': val_pids,\n",
    "        'test': test_pids,\n",
    "    }\n",
    "    if False:\n",
    "        fname = os.path.join(fold_dir, 'ds2pids.pkl')\n",
    "        print(f'Save to: {fname}')\n",
    "        pd.to_pickle(ds2pids, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "seed = 3579\n",
    "remove_pa_in_test = True\n",
    "\n",
    "for i in range(1, n_folds+1):\n",
    "    print('############################################################')\n",
    "    print(f'Processing fold {i}')\n",
    "    experiment = f'fold_{i}'\n",
    "    DATA_DIR = '/media/HSGNN/dataset/V2_2'\n",
    "\n",
    "    PA_edges = pd.read_csv(os.path.join(DATA_DIR, 'PA_edges.csv'), index_col=None)\n",
    "    PC_edges = pd.read_csv(os.path.join(DATA_DIR, 'PC_edges.csv'), index_col=None)\n",
    "    PT_edges = pd.read_csv(os.path.join(DATA_DIR, 'PT_edges.csv'), index_col=None)\n",
    "    cid2cname = pd.read_pickle(os.path.join(PROJ_PATH, 'dataset/dblp_v8/cid2cname.pkl'))\n",
    "    ds2pids = pd.read_pickle(os.path.join(PROJ_PATH, 'dataset', experiment, 'ds2pids.pkl')) \n",
    "\n",
    "    subgraph = MySubGraphs_v2(\n",
    "        PA_edges,\n",
    "        PT_edges,\n",
    "        PC_edges, \n",
    "        cid2cname,\n",
    "        ds2pids,\n",
    "        remove_pa_in_test=remove_pa_in_test,\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "    subgraph.sample_subgraphs()\n",
    "    if False:\n",
    "        sub_G, sub_G_label, mask_G = subgraph.sub_G, subgraph.sub_G_label, subgraph.mask_G\n",
    "        df_node_types, df_edges = subgraph.df_node_types, subgraph.df_edges\n",
    "        id_G = subgraph.id_G\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'subgraphs.pth')\n",
    "        print(f'Save subgraph to {save_path}')\n",
    "        write_subgraph(save_path, sub_G, sub_G_label, mask_G)\n",
    "\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'node_types.csv')\n",
    "        print(f'Save node types to {save_path}')\n",
    "        df_node_types.to_csv(save_path, index=False)\n",
    "\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'edge_list.txt')\n",
    "        print(f'Save edges to {save_path}')\n",
    "        df_edges.to_csv(save_path, header=None, index=None, sep=' ')\n",
    "\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'id.pkl')\n",
    "        print(f'Save id to {save_path}')\n",
    "        pd.to_pickle(id_G, save_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa28543eac014fd73c2548ee912fcba8fde36082445f71f9d493814f53498b12"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
