{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import collections, itertools\n",
    "import networkx as nx\n",
    "import regex as re\n",
    "\n",
    "PROJ_PATH = os.path.join(re.sub(\"/heterogeneous.*$\", '', os.getcwd()), 'heterogeneous_subgraph_representation_for_team_discovery')\n",
    "sys.path.insert(1, os.path.join(PROJ_PATH, 'src'))\n",
    "\n",
    "from train_config import *\n",
    "from datasets import SubgraphDataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors: 6202\n",
      "Number of papers: 4882\n",
      "Number of terms: 2532\n",
      "Number of conf: 21\n",
      "Total number of nodes: 13637\n",
      "Number of P-A edges: 14646\n",
      "Number of P-T edges: 42661\n",
      "Number of P-C edges: 14040\n",
      "Total number of edges: 71347\n"
     ]
    }
   ],
   "source": [
    "# author --> paper --> term --> conference\n",
    "DATA_DIR = '/media/TeamDiscovery/IMDB'\n",
    "PA_edges = pd.read_csv(os.path.join(DATA_DIR, 'PA_edges.csv'), index_col=None)\n",
    "PC_edges = pd.read_csv(os.path.join(DATA_DIR, 'PC_edges.csv'), index_col=None)\n",
    "PT_edges = pd.read_csv(os.path.join(DATA_DIR, 'PT_edges.csv'), index_col=None)\n",
    "# PT_edges = PT_edges[~PT_edges['target'].isin(PC_edges['target'].unique().tolist())]\n",
    "df_edges = pd.concat([PA_edges, PC_edges, PT_edges])\n",
    "\n",
    "no_author = PA_edges['target'].nunique()\n",
    "no_paper = PA_edges['source'].nunique()\n",
    "no_term = PT_edges['target'].nunique()\n",
    "no_conf = PC_edges['target'].nunique()\n",
    "print('Number of authors:', no_author)\n",
    "print('Number of papers:', no_paper)\n",
    "print('Number of terms:', no_term)\n",
    "print('Number of conf:', no_conf)\n",
    "        \n",
    "# Summary\n",
    "nodes = set(df_edges['source'].unique().tolist()+df_edges['target'].unique().tolist())\n",
    "n_nodes = no_author + no_paper + no_term + no_conf\n",
    "print('Total number of nodes:', n_nodes)\n",
    "assert (len(nodes)==n_nodes), 'Incorrect number of nodes'\n",
    "print('Number of P-A edges:', PA_edges.shape[0])\n",
    "print('Number of P-T edges:', PT_edges.shape[0])\n",
    "print('Number of P-C edges:', PC_edges.shape[0])\n",
    "print('Total number of edges:', df_edges.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1     610\n",
       "2    1322\n",
       "3    1569\n",
       "4     947\n",
       "5     339\n",
       "6      73\n",
       "7      18\n",
       "8       4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC_edges.groupby('source', as_index=False).agg({'target': 'count'}).groupby('target').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2059116809116809"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1569+1322)/14040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 6201\n",
      "6202 11083\n",
      "11084 13615\n",
      "13616 13636\n"
     ]
    }
   ],
   "source": [
    "print(min(mapping[mapping['domain']=='author']['serial_id']), max(mapping[mapping['domain']=='author']['serial_id']))\n",
    "print(min(mapping[mapping['domain']=='paper']['serial_id']), max(mapping[mapping['domain']=='paper']['serial_id']))\n",
    "print(min(mapping[mapping['domain']=='term']['serial_id']), max(mapping[mapping['domain']=='term']['serial_id']))\n",
    "print(min(mapping[mapping['domain']=='conf']['serial_id']), max(mapping[mapping['domain']=='conf']['serial_id']),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subgraph\n",
    "# format {SUBGRAPH_IDS}\\t{LABEL}\\t{DATASET}\\n\n",
    "# 304-3229-76-57-946-340-630-632-122-4306-855-1787-4309-555-306-2418-638\tIntellectual\ttrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{13616: 'Action', 13617: 'Adventure', 13618: 'Fantasy', 13619: 'Sci-Fi', 13620: 'Thriller', 13621: 'Documentary', 13622: 'Romance', 13623: 'Animation', 13624: 'Comedy', 13625: 'Family', 13626: 'Musical', 13627: 'Mystery', 13628: 'Western', 13629: 'Drama', 13630: 'History', 13631: 'Sport', 13632: 'Crime', 13633: 'Horror', 13634: 'War', 13635: 'Biography', 13636: 'Music'}\n"
     ]
    }
   ],
   "source": [
    "pd_cid2cname = pd.read_csv('/media/TeamDiscovery/IMDB/labelID_name.csv')\n",
    "cid2cname = dict(zip(pd_cid2cname.Label_ID, pd_cid2cname.Name))\n",
    "print(cid2cname)\n",
    "if False: pd.to_pickle(cid2cname, '/media/TeamDiscovery/IMDB/cid2cname.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySubGraphs():\n",
    "    '''\n",
    "    Args:\n",
    "        cid2cname: dictionary to map conference id to conference name which is used as the label of subgraph\n",
    "        n_subgraphs: number of subgraph for each label\n",
    "        n_nodes_in_subgraph: number of nodes in each subgraph\n",
    "        tvt (default: 60, 20, 20): ratio for training, validate and testing set\n",
    "        shuffle (default=True): whether to shuffle the order of subgraphs\n",
    "    Return:\n",
    "     A subgraph object with:\n",
    "        sub_G: list of subgraphs\n",
    "        sub_G_label: list of subgraphs label\n",
    "        mask: dataset indicator (whether data used for training/validation/testing)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        PA_edges, \n",
    "        PT_edges,\n",
    "        PC_edges,\n",
    "        cid2cname,\n",
    "        n_subgraphs,\n",
    "        n_nodes_in_subgraph,\n",
    "        use_venue=True,\n",
    "        tvt=[60, 20, 20],\n",
    "        shuffle=True):\n",
    "        \n",
    "        assert sum(tvt) == 100, 'Sum of TVT should be 100!'\n",
    "        \n",
    "        self.PA_edges = PA_edges\n",
    "        self.PT_edges = PT_edges\n",
    "        self.PC_edges = PC_edges\n",
    "        self.edges = pd.concat([self.PA_edges, self.PT_edges, self.PC_edges])\n",
    "        \n",
    "        self.cid2cname = cid2cname\n",
    "        self.n_subgraphs = n_subgraphs\n",
    "        self.n_nodes_in_subgraph = n_nodes_in_subgraph\n",
    "        self.use_venue = use_venue\n",
    "        self.tvt = tvt\n",
    "        self.shuffle = shuffle\n",
    "        self.n_groups = len(cid2cname)\n",
    "        \n",
    "    def create_base_graph(self):\n",
    "        '''\n",
    "        Create a base graph for each label before sampling subgraphs\n",
    "            - cid: conference id\n",
    "            - cname: conference name\n",
    "        '''\n",
    "        G_dict = {}\n",
    "        for cid, cname in self.cid2cname.items():\n",
    "            print(f'- Creating base graph for group {cname}')\n",
    "            # filter paper\n",
    "            PC_edges_filtered = self.PC_edges[self.PC_edges['target']==cid]\n",
    "            papers_filtered = list(PC_edges_filtered['source'].unique())\n",
    "            \n",
    "            # filter author\n",
    "            PA_edges_filtered = self.PA_edges[self.PA_edges['source'].isin(papers_filtered)]\n",
    "            \n",
    "            # filter term\n",
    "            PT_edge_filtered = self.PA_edges[self.PA_edges['source'].isin(papers_filtered)]\n",
    "            \n",
    "            # create networkx graph\n",
    "            if self.use_venue:\n",
    "                df_edges_filtered = pd.concat([PC_edges_filtered, PA_edges_filtered, PT_edge_filtered])\n",
    "            else:\n",
    "                df_edges_filtered = pd.concat([PA_edges_filtered, PT_edge_filtered])\n",
    "            G_i = nx.from_pandas_edgelist(df_edges_filtered)\n",
    "            G_dict[cname] = G_i\n",
    "            \n",
    "        return G_dict\n",
    "    \n",
    "    def get_subgraphs_randomly(self, G_dict):\n",
    "        \"\"\"\n",
    "        Randomly generates subgraphs of size n_nodes_in_subgraph\n",
    "        Args\n",
    "            - n_subgraphs (int): number of subgraphs\n",
    "            - n_nodes_in_subgraph (int): number of nodes in each subgraph\n",
    "        Return\n",
    "            - subgraphs (list of lists): list of subgraphs, where each subgraph is a list of nodes\n",
    "        \"\"\"\n",
    "\n",
    "        sub_G = []\n",
    "        sub_G_label = []\n",
    "        random.seed(0)\n",
    "        \n",
    "        print(f'- Sampling {self.n_subgraphs*self.n_groups} subgraphs')\n",
    "        for cname, G in G_dict.items():\n",
    "            print(f'- Sampling {self.n_subgraphs} subgraphs for group {cname}')\n",
    "            for s in range(self.n_subgraphs):\n",
    "                n_nodes = min(len(G.nodes), self.n_nodes_in_subgraph)\n",
    "                sampled_nodes = random.sample(G.nodes, n_nodes)\n",
    "                sub_G.append(sampled_nodes)\n",
    "                sub_G_label.append(cname)\n",
    "                \n",
    "        if self.shuffle:\n",
    "            tmp = list(zip(sub_G, sub_G_label))\n",
    "            random.shuffle(tmp)\n",
    "            sub_G, sub_G_label = zip(*tmp)\n",
    "        return sub_G, sub_G_label\n",
    "     \n",
    "    def get_train_val_test(self):\n",
    "        self.n_samples = self.n_subgraphs * self.n_groups\n",
    "        n_train = int(self.n_samples * self.tvt[0] / 100)\n",
    "        n_val = int(self.n_samples * self.tvt[1] / 100)\n",
    "        n_test = self.n_samples - n_train - n_val\n",
    "        mask = [0] * n_train + [1] * n_val + [2] * n_test\n",
    "        return mask\n",
    "    \n",
    "    def sample_subgraphs(self):\n",
    "        print('Creating base graphs')\n",
    "        self.G_dict = self.create_base_graph()\n",
    "        print('Sampling subgraphs')\n",
    "        self.sub_G, self.sub_G_label = self.get_subgraphs_randomly(self.G_dict)\n",
    "        self.mask = self.get_train_val_test()  \n",
    "\n",
    "        \n",
    "class MySubGraphs_v2():\n",
    "    '''\n",
    "    Args:\n",
    "        cid2cname: dictionary to map conference id to conference name which is used as the label of subgraph\n",
    "        n_subgraphs: number of subgraph for each label\n",
    "        n_nodes_in_subgraph: number of nodes in each subgraph\n",
    "        tvt (default: 60, 20, 20): ratio for training, validate and testing set\n",
    "        shuffle (default=True): whether to shuffle the order of subgraphs\n",
    "    Return:\n",
    "     A subgraph object with:\n",
    "        sub_G: list of subgraphs\n",
    "        sub_G_label: list of subgraphs label\n",
    "        mask: dataset indicator (whether data used for training/validation/testing)\n",
    "    '''\n",
    "    def __init__(\n",
    "        self, \n",
    "        PA_edges,\n",
    "        PT_edges,\n",
    "        PC_edges, \n",
    "        cid2cname,\n",
    "        ds2pids,\n",
    "        remove_pa_in_test=True,\n",
    "        shuffle=True,\n",
    "        seed=0,\n",
    "    ):\n",
    "        \n",
    "        self.PA_edges = PA_edges\n",
    "        self.PT_edges = PT_edges\n",
    "        self.PC_edges = PC_edges\n",
    "        self.paper_nodes = list(PA_edges['source'].unique())\n",
    "        self.author_nodes = list(PA_edges['target'].unique())\n",
    "        self.term_nodes = list(PT_edges['target'].unique())\n",
    "        self.conference_nodes = list(PC_edges['target'].unique())\n",
    "        \n",
    "        self.cid2cname = cid2cname # conference id to conference name\n",
    "        self.ds2pids = ds2pids\n",
    "        self.train_pids = ds2pids['train']\n",
    "        self.val_pids = ds2pids['val']\n",
    "        self.test_pids = ds2pids['test']\n",
    "        self.remove_pa_in_test = remove_pa_in_test\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.n_groups = len(cid2cname)\n",
    "        print(f'Number of train: {len(self.train_pids)}')\n",
    "        print(f'Number of val: {len(self.val_pids)}')\n",
    "        print(f'Number of test: {len(self.test_pids)}')\n",
    "        \n",
    "    def get_subgraphs_based_on_paper(self):\n",
    "        \"\"\"\n",
    "        Randomly generates subgraphs of size n_nodes_in_subgraph\n",
    "        Args\n",
    "            - n_subgraphs (int): number of subgraphs\n",
    "            - n_nodes_in_subgraph (int): number of nodes in each subgraph\n",
    "        Return\n",
    "            - subgraphs (list of lists): list of subgraphs, where each subgraph is a list of nodes\n",
    "        \"\"\"\n",
    "        random.seed(self.seed)\n",
    "        \n",
    "        sub_G = []\n",
    "        sub_G_label = []\n",
    "        mask_G = []\n",
    "        id_G = []\n",
    "        \n",
    "        print(f'- Sampling {len(self.paper_nodes)} subgraphs')\n",
    "        for pid in self.paper_nodes:\n",
    "            PC_edges_filtered = self.PC_edges[self.PC_edges['source']==pid]\n",
    "            cids = PC_edges_filtered['target'].unique().tolist()\n",
    "            cnames = '-'.join([cid2cname[i].replace('-', '') for i in cids])\n",
    "            \n",
    "            sampled_paper_nodes = [pid]\n",
    "            id_G.append(pid) # for prediction phase\n",
    "            sampled_term_nodes = list(self.PT_edges[self.PT_edges['source'].isin(sampled_paper_nodes)]['target'].unique())\n",
    "            sampled_author_nodes = list(self.PA_edges[self.PA_edges['source'].isin(sampled_paper_nodes)]['target'].unique())\n",
    "         \n",
    "            if pid in self.test_pids and self.remove_pa_in_test:\n",
    "                sampled_nodes = sampled_paper_nodes + sampled_term_nodes\n",
    "            else:        \n",
    "                sampled_nodes = sampled_author_nodes + sampled_paper_nodes + sampled_term_nodes\n",
    "\n",
    "            sub_G.append(sampled_nodes)\n",
    "            sub_G_label.append(cnames)\n",
    "\n",
    "            if pid in self.train_pids:\n",
    "                mask_G.append(0)\n",
    "            elif pid in self.val_pids:\n",
    "                mask_G.append(1)\n",
    "            elif pid in self.test_pids:\n",
    "                mask_G.append(2)\n",
    "\n",
    "        if self.shuffle:\n",
    "            print('-- Shuffle data')\n",
    "            tmp = list(zip(sub_G, sub_G_label, mask_G, id_G))\n",
    "            random.shuffle(tmp)\n",
    "            tmp = sorted(tmp, key=lambda x: x[2])\n",
    "            sub_G, sub_G_label, mask_G, id_G = zip(*tmp)\n",
    "        return sub_G, sub_G_label, mask_G, id_G\n",
    "    \n",
    "    def get_node_types(self):\n",
    "        # author --> paper --> term --> conference\n",
    "        no_author = len(self.author_nodes)\n",
    "        no_paper = len(self.paper_nodes)\n",
    "        no_term = len(self.term_nodes)\n",
    "        node_id = range(no_author + no_paper + no_term)\n",
    "        node_type = [0] * no_author + [1] * no_paper + [2] * no_term\n",
    "        nodetype_mapping = {\n",
    "            0: 'author',\n",
    "            1: 'paper',\n",
    "            2: 'term',\n",
    "        }\n",
    "        df_node_types = pd.DataFrame({'node_id': node_id, 'node_type': node_type})\n",
    "        df_node_types['node_type_name'] = df_node_types['node_type'].map(nodetype_mapping)\n",
    "        return df_node_types\n",
    "\n",
    "    def get_edges(self):\n",
    "        if self.remove_pa_in_test:\n",
    "            self.PA_edges_filtered = self.PA_edges[~self.PA_edges['target'].isin(self.test_pids)]\n",
    "        else:\n",
    "            self.PA_edges_filtered = self.PA_edges.copy()\n",
    "        df_edges = pd.concat([self.PT_edges, self.PA_edges_filtered])\n",
    "        print(f'Number of edges {df_edges.shape[0]}')\n",
    "        return df_edges\n",
    "        \n",
    "    def sample_subgraphs(self):\n",
    "        print('Start sampling subgraphs ...')\n",
    "        self.sub_G, self.sub_G_label, self.mask_G, self.id_G = self.get_subgraphs_based_on_paper()\n",
    "        \n",
    "        print('Start getting nodes ...')\n",
    "        self.df_node_types = self.get_node_types()\n",
    "        \n",
    "        print('Start getting edges ...')\n",
    "        self.df_edges = self.get_edges()\n",
    "                 \n",
    "def write_subgraph(sub_f, sub_G, sub_G_label, mask):\n",
    "    \"\"\"\n",
    "    Write subgraph information into the appropriate format for HSGNN (tab-delimited file where each row\n",
    "    has dash-delimited nodes, subgraph label, and train/val/test label).\n",
    "    Args\n",
    "        - sub_f (str): file directory to save subgraph information\n",
    "        - sub_G (list of lists): list of subgraphs, where each subgraph is a list of nodes\n",
    "        - sub_G_label (list): subgraph labels\n",
    "        - mask (list): 0 if subgraph is in train set, 1 if in val set, 2 if in test set\n",
    "    \"\"\"\n",
    "\n",
    "    with open(sub_f, \"w\") as fout:\n",
    "        for g, l, m in zip(sub_G, sub_G_label, mask):\n",
    "            g = [str(val) for val in g]\n",
    "            if len(g) == 0: continue\n",
    "            if m == 0: fout.write(\"\\t\".join([\"-\".join(g), str(l), \"train\", \"\\n\"]))\n",
    "            elif m == 1: fout.write(\"\\t\".join([\"-\".join(g), str(l), \"val\", \"\\n\"]))\n",
    "            elif m == 2: fout.write(\"\\t\".join([\"-\".join(g), str(l), \"test\", \"\\n\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n"
     ]
    }
   ],
   "source": [
    "folds = pd.read_pickle('/media/TeamDiscovery/IMDB/imdb_train_test_indices.pkl')\n",
    "entityID_map = np.genfromtxt('/media/TeamDiscovery/IMDB/entity_id_mapping.csv', delimiter=\",\", dtype=str)\n",
    "paperIDmap = {str(i[2]):int(i[3]) for i in entityID_map[1:] if i[1]==\"paper\"}\n",
    "print(folds.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Prepare tvt file for fold: 1\n",
      "Number of train: 4174\n",
      "Number of val: 219\n",
      "Number of test: 489\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_1/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 2\n",
      "Number of train: 4174\n",
      "Number of val: 219\n",
      "Number of test: 489\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_2/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 3\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_3/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 4\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_4/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 5\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_5/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 6\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_6/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 7\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_7/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 8\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_8/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 9\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_9/ds2pids.pkl\n",
      "############################################################\n",
      "Prepare tvt file for fold: 10\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Save to: /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_10/ds2pids.pkl\n"
     ]
    }
   ],
   "source": [
    "seed = 3579\n",
    "random.seed(seed)\n",
    "val_ratio = 0.05\n",
    "n_folds = 10\n",
    "for i in range(1, n_folds+1):\n",
    "    print('############################################################')\n",
    "    print(f'Prepare tvt file for fold: {i}')\n",
    "    experiment = f'imdb_fold_{i}'\n",
    "    train_ids_origin = folds[i]['Train']\n",
    "    test_ids_origin = folds[i]['Test']\n",
    "    trainval_ids = [paperIDmap[idx] for idx in train_ids_origin]\n",
    "    test_pids = [paperIDmap[idx] for idx in test_ids_origin]\n",
    "    random.shuffle(trainval_ids)\n",
    "    n_trainval = len(trainval_ids)\n",
    "    n_val = int(n_trainval * val_ratio)\n",
    "    train_pids = trainval_ids[:-n_val]\n",
    "    val_pids = trainval_ids[-n_val:]\n",
    "    \n",
    "    assert min(train_pids+val_pids+test_pids) == 6202\n",
    "    assert max(train_pids+val_pids+test_pids) == (6202+4882-1)\n",
    "    assert len(set(train_pids+val_pids+test_pids)) == 4882\n",
    "    \n",
    "    fold_dir = os.path.join(PROJ_PATH, 'dataset', experiment)\n",
    "    if not os.path.exists(fold_dir): \n",
    "        os.mkdir(fold_dir)\n",
    "    print(f'Number of train: {len(train_pids)}')\n",
    "    print(f'Number of val: {len(val_pids)}')\n",
    "    print(f'Number of test: {len(test_pids)}')\n",
    "    ds2pids = {\n",
    "        'train': train_pids,\n",
    "        'val': val_pids,\n",
    "        'test': test_pids,\n",
    "    }\n",
    "    if False:\n",
    "        fname = os.path.join(fold_dir, 'ds2pids.pkl')\n",
    "        print(f'Save to: {fname}')\n",
    "        pd.to_pickle(ds2pids, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################################\n",
      "Processing fold 1\n",
      "Number of train: 4174\n",
      "Number of val: 219\n",
      "Number of test: 489\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_1/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_1/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_1/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_1/id.pkl\n",
      "############################################################\n",
      "Processing fold 2\n",
      "Number of train: 4174\n",
      "Number of val: 219\n",
      "Number of test: 489\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_2/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_2/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_2/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_2/id.pkl\n",
      "############################################################\n",
      "Processing fold 3\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_3/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_3/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_3/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_3/id.pkl\n",
      "############################################################\n",
      "Processing fold 4\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_4/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_4/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_4/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_4/id.pkl\n",
      "############################################################\n",
      "Processing fold 5\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_5/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_5/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_5/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_5/id.pkl\n",
      "############################################################\n",
      "Processing fold 6\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_6/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_6/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_6/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_6/id.pkl\n",
      "############################################################\n",
      "Processing fold 7\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_7/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_7/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_7/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_7/id.pkl\n",
      "############################################################\n",
      "Processing fold 8\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_8/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_8/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_8/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_8/id.pkl\n",
      "############################################################\n",
      "Processing fold 9\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_9/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_9/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_9/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_9/id.pkl\n",
      "############################################################\n",
      "Processing fold 10\n",
      "Number of train: 4175\n",
      "Number of val: 219\n",
      "Number of test: 488\n",
      "Start sampling subgraphs ...\n",
      "- Sampling 4882 subgraphs\n",
      "-- Shuffle data\n",
      "Start getting nodes ...\n",
      "Start getting edges ...\n",
      "Number of edges 57307\n",
      "Save subgraph to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_10/subgraphs.pth\n",
      "Save node types to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_10/node_types.csv\n",
      "Save edges to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_10/edge_list.txt\n",
      "Save id to /home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/imdb_fold_10/id.pkl\n"
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "seed = 3579\n",
    "remove_pa_in_test = True\n",
    "\n",
    "for i in range(1, n_folds+1):\n",
    "    print('############################################################')\n",
    "    print(f'Processing fold {i}')\n",
    "    experiment = f'imdb_fold_{i}'\n",
    "    DATA_DIR = '/media/TeamDiscovery/IMDB'\n",
    "\n",
    "    PA_edges = pd.read_csv(os.path.join(DATA_DIR, 'PA_edges.csv'), index_col=None)\n",
    "    PC_edges = pd.read_csv(os.path.join(DATA_DIR, 'PC_edges.csv'), index_col=None)\n",
    "    PT_edges = pd.read_csv(os.path.join(DATA_DIR, 'PT_edges.csv'), index_col=None)\n",
    "    cid2cname = pd.read_pickle(os.path.join(DATA_DIR, 'cid2cname.pkl'))\n",
    "    ds2pids = pd.read_pickle(os.path.join(PROJ_PATH, 'dataset', experiment, 'ds2pids.pkl')) \n",
    "\n",
    "    subgraph = MySubGraphs_v2(\n",
    "        PA_edges,\n",
    "        PT_edges,\n",
    "        PC_edges, \n",
    "        cid2cname,\n",
    "        ds2pids,\n",
    "        remove_pa_in_test=remove_pa_in_test,\n",
    "        shuffle=True,\n",
    "        seed=seed,\n",
    "    )\n",
    "    subgraph.sample_subgraphs()\n",
    "    if True:\n",
    "        sub_G, sub_G_label, mask_G = subgraph.sub_G, subgraph.sub_G_label, subgraph.mask_G\n",
    "        df_node_types, df_edges = subgraph.df_node_types, subgraph.df_edges\n",
    "        id_G = subgraph.id_G\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'subgraphs.pth')\n",
    "        print(f'Save subgraph to {save_path}')\n",
    "        write_subgraph(save_path, sub_G, sub_G_label, mask_G)\n",
    "\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'node_types.csv')\n",
    "        print(f'Save node types to {save_path}')\n",
    "        df_node_types.to_csv(save_path, index=False)\n",
    "\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'edge_list.txt')\n",
    "        print(f'Save edges to {save_path}')\n",
    "        df_edges.to_csv(save_path, header=None, index=None, sep=' ')\n",
    "\n",
    "        save_path = os.path.join(PROJ_PATH, 'dataset', experiment, 'id.pkl')\n",
    "        print(f'Save id to {save_path}')\n",
    "        pd.to_pickle(id_G, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PA_edges.csv\r\n",
      "PC_edges.csv\r\n",
      "PT_edges.csv\r\n",
      "ae_t2v_dim300_tSkill_dataset_IMDB.pkl\r\n",
      "ae_t2v_dim300_tUser_dataset_IMDB.pkl\r\n",
      "ae_t2v_dimSkill300_dimUser300_tFull_dataset_IMDB.pkl\r\n",
      "entity_id_mapping.csv\r\n",
      "imdb_mappings\r\n",
      "imdb_train_test_indices.pkl\r\n",
      "labelID_name.csv\r\n",
      "preprocess\r\n"
     ]
    }
   ],
   "source": [
    "!ls /media/TeamDiscovery/IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa28543eac014fd73c2548ee912fcba8fde36082445f71f9d493814f53498b12"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
