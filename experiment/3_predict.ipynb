{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from collections import Counter\n",
    "import collections, itertools\n",
    "import networkx as nx\n",
    "import regex as re\n",
    "\n",
    "PROJ_PATH = os.path.join(re.sub(\"/heterogeneous.*$\", '', os.getcwd()), 'heterogeneous_subgraph_representation_for_team_discovery')\n",
    "sys.path.insert(1, os.path.join(PROJ_PATH, 'src'))\n",
    "\n",
    "from train_config import *\n",
    "from datasets import SubgraphDataset\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import torch\n",
    "import train as tr\n",
    "import subgraph_utils\n",
    "from pathlib import Path\n",
    "from src import HSGNN\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from datasets import SubgraphDataset\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceAgent:\n",
    "    def __init__(self,\n",
    "                 common_args={},\n",
    "                 pred_args={'task':'dblp_v82',\n",
    "                            'restoreModelPath':'',\n",
    "                            'restoreModelName':''},\n",
    "                 device='cuda',\n",
    "                 dropout=False,\n",
    "                ):\n",
    "        default_common_args = {\n",
    "            \"max_epochs\" : 300,\n",
    "            \"tb_dir\" : os.path.join(PROJ_PATH, 'dataset/tensorboard/'),\n",
    "            'trial': None,\n",
    "            'runTest': True,\n",
    "            \"no_checkpointing\" : False, #0 and True or 1 and False\n",
    "            \"runTest\": True, \n",
    "            \"no_save\": True,\n",
    "            \"debug_mode\": False,\n",
    "            \"subset_data\": False,\n",
    "            \"noTrain\": True,\n",
    "            \"log_path\": None,\n",
    "        }\n",
    "        default_common_args.update(common_args)\n",
    "        common_args = default_common_args\n",
    "        dict_args = {**common_args, **pred_args}\n",
    "        args = Namespace(**dict_args)\n",
    "        args.tb_name = 'S_' + args.task + '_optuna'\n",
    "        args.config_path = os.path.join(PROJ_PATH, 'HSGNN/config_files/', args.task, 'config.json')\n",
    "        \n",
    "       \n",
    "        self.model, self.hyperparameters = tr.build_model(args)\n",
    "        self.trainer, _, _ = tr.build_trainer(args, self.hyperparameters)\n",
    "\n",
    "        run_config = read_json(args.config_path)\n",
    "        run_config['no_cuda'] = True\n",
    "        if 'local' in run_config['tb'] and run_config['tb']['local']:\n",
    "            run_config['tb']['dir_full'] = run_config['tb']['dir']\n",
    "        else:\n",
    "            run_config['tb']['dir_full'] = os.path.join(config.PROJECT_ROOT, run_config['tb']['dir'])\n",
    "#         trainer, trainer_kwargs, results_path = tr.build_trainer(args, hyperparameters)\n",
    "\n",
    "        random.seed(self.hyperparameters['seed'])\n",
    "        torch.manual_seed(self.hyperparameters['seed'])\n",
    "        np.random.seed(self.hyperparameters['seed'])\n",
    "        torch.cuda.manual_seed(self.hyperparameters['seed'])\n",
    "        torch.cuda.manual_seed_all(self.hyperparameters['seed']) \n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "        map_location = lambda storage, loc: storage.cuda()\n",
    "        checkpoint = torch.load(Path(args.restoreModelPath)/args.restoreModelName,\n",
    "                               map_location=map_location)\n",
    "        self.model_dict = self.model.state_dict()\n",
    "        self.pretrain_dict = {k: v for k, v in checkpoint['state_dict'].items() if k in self.model_dict}\n",
    "        self.model.load_state_dict(self.pretrain_dict)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train_dataset(self):\n",
    "        print('TRAIN DATASET')\n",
    "        self.model.prepare_data()\n",
    "        dataset = SubgraphDataset(\n",
    "            self.model.train_sub_G, self.model.train_sub_G_label, self.model.train_cc_ids,\n",
    "            self.model.train_N_border, self.model.train_neigh_pos_similarities, self.model.train_int_struc_similarities,\n",
    "            self.model.train_bor_struc_similarities, self.model.multilabel, self.model.multilabel_binarizer)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=len(dataset), shuffle=False, collate_fn=self.model._pad_collate)\n",
    "        return loader\n",
    "\n",
    "    def val_dataset(self):\n",
    "        print('VAL DATASET')\n",
    "        dataset = SubgraphDataset(\n",
    "            self.model.val_sub_G, self.model.val_sub_G_label, self.model.val_cc_ids,\n",
    "            self.model.val_N_border, self.model.val_neigh_pos_similarities, self.model.val_int_struc_similarities,\n",
    "            self.model.val_bor_struc_similarities, self.model.multilabel, self.model.multilabel_binarizer)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=len(dataset), shuffle=False, collate_fn=self.model._pad_collate)\n",
    "        return loader\n",
    "\n",
    "    def test_dataset(self):\n",
    "        self.model.prepare_test_data()\n",
    "        print('TEST DATASET')\n",
    "        dataset = SubgraphDataset(\n",
    "            self.model.test_sub_G, self.model.test_sub_G_label, self.model.test_cc_ids,\n",
    "            self.model.test_N_border, self.model.test_neigh_pos_similarities, self.model.test_int_struc_similarities,\n",
    "            self.model.test_bor_struc_similarities, self.model.multilabel, self.model.multilabel_binarizer)\n",
    "        loader = DataLoader(\n",
    "            dataset, batch_size=len(dataset), shuffle=False, collate_fn=self.model._pad_collate)\n",
    "        return loader\n",
    "    \n",
    "    def single_dataset_inference(self, loader, dataset_type='train'):\n",
    "        for i, ds in enumerate(loader):\n",
    "            subgraph_ids = ds['subgraph_ids']\n",
    "            cc_ids = ds['cc_ids']\n",
    "            subgraph_idx = ds['subgraph_idx']\n",
    "            labels = ds['label'].squeeze(-1)\n",
    "            # get similarities for batch\n",
    "            NP_sim = ds['NP_sim']\n",
    "            I_S_sim = ds['I_S_sim']\n",
    "            B_S_sim = ds['B_S_sim']\n",
    "        if dataset_type == 'train':\n",
    "            N_I_cc_embed = self.model.train_N_I_cc_embed\n",
    "            N_B_cc_embed = self.model.train_N_B_cc_embed\n",
    "            S_I_cc_embed = self.model.train_S_I_cc_embed\n",
    "            S_B_cc_embed = self.model.train_S_B_cc_embed\n",
    "            P_I_cc_embed = self.model.train_P_I_cc_embed\n",
    "            P_B_cc_embed = self.model.train_P_B_cc_embed\n",
    "        elif dataset_type == 'val':\n",
    "            N_I_cc_embed = self.model.val_N_I_cc_embed\n",
    "            N_B_cc_embed = self.model.val_N_B_cc_embed\n",
    "            S_I_cc_embed = self.model.val_S_I_cc_embed\n",
    "            S_B_cc_embed = self.model.val_S_B_cc_embed\n",
    "            P_I_cc_embed = self.model.val_P_I_cc_embed\n",
    "            P_B_cc_embed = self.model.val_P_B_cc_embed\n",
    "        elif dataset_type == 'test':\n",
    "            N_I_cc_embed = self.model.test_N_I_cc_embed\n",
    "            N_B_cc_embed = self.model.test_N_B_cc_embed\n",
    "            S_I_cc_embed = self.model.test_S_I_cc_embed\n",
    "            S_B_cc_embed = self.model.test_S_B_cc_embed\n",
    "            P_I_cc_embed = self.model.test_P_I_cc_embed\n",
    "            P_B_cc_embed = self.model.test_P_B_cc_embed\n",
    "        if self.dropout:\n",
    "            embedding = self.model.myforward(\n",
    "                dataset_type, N_I_cc_embed, N_B_cc_embed,\n",
    "                S_I_cc_embed, S_B_cc_embed, P_I_cc_embed,\n",
    "                P_B_cc_embed,\n",
    "                subgraph_ids.to(self.device), \n",
    "                cc_ids.to(self.device), \n",
    "                subgraph_idx.to(self.device), \n",
    "                NP_sim, \n",
    "                I_S_sim.to(self.device), \n",
    "                B_S_sim.to(self.device),\n",
    "            )\n",
    "        else:\n",
    "            embedding = self.model.myforward_2(\n",
    "                dataset_type, N_I_cc_embed, N_B_cc_embed,\n",
    "                S_I_cc_embed, S_B_cc_embed, P_I_cc_embed,\n",
    "                P_B_cc_embed,\n",
    "                subgraph_ids.to(self.device), \n",
    "                cc_ids.to(self.device), \n",
    "                subgraph_idx.to(self.device), \n",
    "                NP_sim, \n",
    "                I_S_sim.to(self.device), \n",
    "                B_S_sim.to(self.device),\n",
    "            )\n",
    "        output = embedding.cpu().detach().numpy().tolist()\n",
    "        print(f'Number of samples {len(output)}')\n",
    "        return output\n",
    "    \n",
    "    def inference(self):\n",
    "        train_ds = self.train_dataset()\n",
    "        val_ds = self.val_dataset()\n",
    "        test_ds = self.test_dataset()\n",
    "        self.model.eval()\n",
    "        output = []\n",
    "        with torch.no_grad():\n",
    "            # train\n",
    "            output += self.single_dataset_inference(train_ds, dataset_type='train')\n",
    "            output += self.single_dataset_inference(val_ds, dataset_type='val')\n",
    "            output += self.single_dataset_inference(test_ds, dataset_type='test')\n",
    "        self.output = output\n",
    "        \n",
    "def save_embedding(output, ds_name, output_path, save_name='pid2vec_128.pkl'):\n",
    "    ds2pids = pd.read_pickle(os.path.join(PROJ_PATH, 'dataset', ds_name, 'ds2pids.pkl')) \n",
    "    pid = pd.read_pickle(os.path.join('/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset', ds_name, 'id.pkl')) \n",
    "    \n",
    "    assert len(output) == len(pid)\n",
    "    pid2vec = dict(zip(pid, output))\n",
    "    \n",
    "    if save_name is not None:\n",
    "        if not os.path.exists(output_path):\n",
    "            os.mkdir(output_path)\n",
    "        save_path = os.path.join(output_path, save_name)\n",
    "        print(f'Save to {save_path}')\n",
    "        pd.to_pickle(pid2vec, save_path)\n",
    "    return pid2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_args_fold_1 = {\n",
    "    \"task\" : 'fold_1',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_1_optuna/version_5',\n",
    "    \"restoreModelName\": 'epoch=6-val_micro_f1=0.31-val_acc=0.31-val_auroc=0.86.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_2 = {\n",
    "    \"task\" : 'fold_2',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_2_optuna/version_4',\n",
    "    \"restoreModelName\": 'epoch=2-val_micro_f1=0.34-val_acc=0.34-val_auroc=0.87.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_3 = {\n",
    "    \"task\" : 'fold_3',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_3_optuna/version_8',\n",
    "    \"restoreModelName\": 'epoch=3-val_micro_f1=0.34-val_acc=0.34-val_auroc=0.89.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_4 = {\n",
    "    \"task\" : 'fold_4',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_4_optuna/version_5',\n",
    "    \"restoreModelName\": 'epoch=4-val_micro_f1=0.32-val_acc=0.32-val_auroc=0.88.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_5 = {\n",
    "    \"task\" : 'fold_5',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_5_optuna/version_6',\n",
    "    \"restoreModelName\": 'epoch=4-val_micro_f1=0.30-val_acc=0.30-val_auroc=0.87.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_6 = {\n",
    "    \"task\" : 'fold_6',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_6_optuna/version_6',\n",
    "    \"restoreModelName\": 'epoch=3-val_micro_f1=0.32-val_acc=0.32-val_auroc=0.89.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_7 = {\n",
    "    \"task\" : 'fold_7',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_7_optuna/version_3',\n",
    "    \"restoreModelName\": 'epoch=4-val_micro_f1=0.31-val_acc=0.31-val_auroc=0.88.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_8 = {\n",
    "    \"task\" : 'fold_8',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_8_optuna/version_4',\n",
    "    \"restoreModelName\": 'epoch=2-val_micro_f1=0.34-val_acc=0.34-val_auroc=0.87.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_9 = {\n",
    "    \"task\" : 'fold_9',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_9_optuna/version_9',\n",
    "    \"restoreModelName\": 'epoch=3-val_micro_f1=0.34-val_acc=0.34-val_auroc=0.88.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_10 = {\n",
    "    \"task\" : 'fold_10',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/fold_10_optuna/version_9',\n",
    "    \"restoreModelName\": 'epoch=4-val_micro_f1=0.33-val_acc=0.33-val_auroc=0.88.ckpt',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_args = [\n",
    "    pred_args_fold_1, pred_args_fold_2, pred_args_fold_3, \n",
    "    pred_args_fold_4, pred_args_fold_5, \n",
    "    pred_args_fold_6, \n",
    "    pred_args_fold_7, pred_args_fold_8, pred_args_fold_9, pred_args_fold_10,\n",
    "]\n",
    "for i, a in enumerate(ls_args):\n",
    "    print('#####################################################################################')\n",
    "    pred_args = a.copy()\n",
    "    agent = InferenceAgent(pred_args=pred_args, dropout=True)\n",
    "    agent.inference()\n",
    "    output = agent.output\n",
    "    output_path = '/media/HSGNN/output'\n",
    "    M = 'singlepaper'\n",
    "    D = pred_args['task']\n",
    "    S = '128'\n",
    "    F = pred_args['restoreModelName'].split('.ckpt')[0]\n",
    "    save_name = 'M={}_D={}_S={}_F={}.pkl'.format(M, D, S, F)\n",
    "    pid2vec = save_embedding(output, ds_name=D, output_path=output_path, save_name=save_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_args_fold_1 = {\n",
    "    \"task\" : 'imdb_fold_1',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_1/version_2532099',\n",
    "    \"restoreModelName\": 'epoch=71-val_micro_f1=0.99-val_acc=0.95-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_2 = {\n",
    "    \"task\" : 'imdb_fold_2',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_2/version_8961335',\n",
    "    \"restoreModelName\": 'epoch=95-val_micro_f1=0.98-val_acc=0.94-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_3 = {\n",
    "    \"task\" : 'imdb_fold_3',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_3/version_4916345',\n",
    "    \"restoreModelName\": 'epoch=73-val_micro_f1=0.98-val_acc=0.94-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "pred_args_fold_4 = {\n",
    "    \"task\" : 'imdb_fold_4',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_4/version_5017442',\n",
    "    \"restoreModelName\": 'epoch=59-val_micro_f1=0.99-val_acc=0.95-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_5 = {\n",
    "    \"task\" : 'imdb_fold_5',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_5/version_7655999',\n",
    "    \"restoreModelName\": 'epoch=73-val_micro_f1=0.98-val_acc=0.93-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_6 = {\n",
    "    \"task\" : 'imdb_fold_6',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_6/version_8773398',\n",
    "    \"restoreModelName\": 'epoch=82-val_micro_f1=0.99-val_acc=0.95-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_7 = {\n",
    "    \"task\" : 'imdb_fold_7',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_7/version_7815933',\n",
    "    \"restoreModelName\": 'epoch=96-val_micro_f1=0.99-val_acc=0.94-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_8 = {\n",
    "    \"task\" : 'imdb_fold_8',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_8/version_9421881',\n",
    "    \"restoreModelName\": 'epoch=90-val_micro_f1=0.98-val_acc=0.90-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_9 = {\n",
    "    \"task\" : 'imdb_fold_9',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_9/version_5985613',\n",
    "    \"restoreModelName\": 'epoch=61-val_micro_f1=0.99-val_acc=0.94-val_auroc=1.00.ckpt',\n",
    "}\n",
    "\n",
    "\n",
    "pred_args_fold_10 = {\n",
    "    \"task\" : 'imdb_fold_10',\n",
    "    \"restoreModelPath\" : '/home/hoang/github/heterogeneous_subgraph_representation_for_team_discovery/dataset/tensorboard/imdb_fold_10/version_4329351',\n",
    "    \"restoreModelName\": 'epoch=93-val_micro_f1=0.99-val_acc=0.96-val_auroc=1.00.ckpt',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_args = [\n",
    "    pred_args_fold_1, pred_args_fold_2, pred_args_fold_3, \n",
    "    pred_args_fold_4, pred_args_fold_5, pred_args_fold_6, \n",
    "    pred_args_fold_7, pred_args_fold_8, pred_args_fold_9, \n",
    "    pred_args_fold_10,\n",
    "]\n",
    "for i, a in enumerate(ls_args):\n",
    "    print('#####################################################################################')\n",
    "    pred_args = a.copy()\n",
    "    agent = InferenceAgent(pred_args=pred_args, dropout=True)\n",
    "    agent.inference()\n",
    "    output = agent.output\n",
    "    output_path = '/media/HSGNN/output'\n",
    "    M = 'singlepaper'\n",
    "    D = pred_args['task']\n",
    "    S = '128'\n",
    "    F = pred_args['restoreModelName'].split('.ckpt')[0]\n",
    "    save_name = 'M={}_D={}_S={}_F={}.pkl'.format(M, D, S, F)\n",
    "    pid2vec = save_embedding(output, ds_name=D, output_path=output_path, save_name=save_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fa28543eac014fd73c2548ee912fcba8fde36082445f71f9d493814f53498b12"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229.004px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
